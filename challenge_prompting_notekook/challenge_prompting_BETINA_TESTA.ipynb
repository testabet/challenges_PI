{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92de8ca4-7181-45f5-9980-b723d2566f5e",
   "metadata": {},
   "source": [
    "# Challenge Prompting\n",
    "\n",
    "Resolver los siguientes ejercicios dejando el codigo con su ejecucion.\n",
    "\n",
    "Importar las librerias necesarias y **correr las celdas para visualizar el resultado en cada ejercicio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65bbb09-0f5a-4e97-9a06-2361c5cdd1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bloque importacion de librerias\n",
    "\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e004e5b7-b704-4592-b8b4-b01b8d6687cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## bloque variables de entorno\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"True\")  # Verify the key is loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac79e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ej=\"\"\"La paciente, Mar√≠a Gonz√°lez, de 45 a√±os, fue admitida en el Hospital Central el 5 de agosto de 2023 debido a s√≠ntomas de fatiga cr√≥nica y dolores musculares./\n",
    "Tras una serie de an√°lisis. La doctora a cargo, Laura Ram√≠rez, recomend√≥ un tratamiento basado en fisioterapia y medicamentos analg√©sicos. /\n",
    "La pr√≥xima consulta est√° programada para el 15 de septiembre.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6745ff-cd45-4231-b3d6-518954a9ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bloque conexion a Cohere\n",
    "import cohere\n",
    "co = cohere.ClientV2()\n",
    "# alternativa:\n",
    "# co = cohere.ClientV2(api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aacdb26-ce51-49cc-b37f-5aa45c09ff51",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Extraccion de entidades\n",
    "\n",
    "Utilizar el LLM para extraer las siguientes entidades del texto medico.\n",
    "\n",
    "- Paciente:\n",
    "    - Nombre\n",
    "    - Edad\n",
    "- Fecha de admisi√≥n\n",
    "- S√≠ntomas\n",
    "- Diagn√≥stico\n",
    "- Tratamiento recomendado\n",
    "\n",
    "**Aclaracion:** \n",
    "\n",
    "La salida tiene que ser un **string con formato de tipo json**, el cual se convertira en un diccionario de Python.\n",
    "\n",
    "Si la linea de conversion en test da error el ejercicio no esta completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "750e43d7-b074-4973-9cd0-5a6fbe816084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paciente': {'nombre': 'Mar√≠a Gonz√°lez', 'edad': 45},\n",
       " 'fecha_admision': '2023-08-05',\n",
       " 'sintomas': ['fatiga cr√≥nica', 'dolores musculares'],\n",
       " 'diagnostico': 'fibromialgia',\n",
       " 'tratamiento': ['fisioterapia', 'medicamentos analg√©sicos']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo \n",
    "\n",
    "# texto a analizar\n",
    "\"\"\"La paciente, Mar√≠a Gonz√°lez, de 45 a√±os, fue admitida en el Hospital Central el 5 de agosto de 2023 debido a s√≠ntomas de fatiga cr√≥nica y dolores musculares./\n",
    "Tras una serie de an√°lisis, se diagnostic√≥ fibromialgia. La doctora a cargo, Laura Ram√≠rez, recomend√≥ un tratamiento basado en fisioterapia y medicamentos analg√©sicos. /\n",
    "La pr√≥xima consulta est√° programada para el 15 de septiembre.\"\"\"\n",
    "\n",
    "\n",
    "# respuesta del LLM\n",
    "{\n",
    "  \"paciente\": {\n",
    "    \"nombre\": \"Mar√≠a Gonz√°lez\",\n",
    "    \"edad\": 45\n",
    "  },\n",
    "  \"fecha_admision\": \"2023-08-05\",\n",
    "  \"sintomas\": [\n",
    "    \"fatiga cr√≥nica\",\n",
    "    \"dolores musculares\"\n",
    "  ],\n",
    "  \"diagnostico\": \"fibromialgia\",\n",
    "  \"tratamiento\": [\n",
    "    \"fisioterapia\",\n",
    "    \"medicamentos analg√©sicos\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "425cffab-9efd-4d80-bc64-6ef69ce233e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_analize = \"\"\"Sof√≠a L√≥pez, de 28 a√±os, ingres√≥ al Hospital Infantil el 3 de abril de 2023 debido a fiebre alta y tos persistente./\n",
    "Despu√©s de varias pruebas, se le diagnostic√≥ neumon√≠a. La pediatra responsable, Dra. Claudia Torres, indic√≥ tratamiento con antibi√≥ticos y reposo./\n",
    "La pr√≥xima evaluaci√≥n ser√° el 10 de abril.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ff6d292-d4cb-4484-811e-d0d641c66d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paciente\": {\n",
      "    \"nombre\": \"Sof√≠a L√≥pez\",\n",
      "    \"edad\": \"28 a√±os\"\n",
      "  },\n",
      "  \"fecha_admision\": \"03/04/2023\",\n",
      "  \"sintomas\": [\n",
      "    \"Fiebre alta\",\n",
      "    \"Tos persistente\"\n",
      "  ],\n",
      "  \"diagnostico\": \"Neumon√≠a\",\n",
      "  \"tratamiento\": [\n",
      "    \"Antibi√≥ticos\",\n",
      "    \"Reposo\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "co = cohere.ClientV2()\n",
    "response = co.chat(\n",
    "    model= \"command-r-plus-08-2024\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"Sos un asistente capaza de interpretar entidades en un texto medico. Extrae el nombre y edad del paciente, la fecha de admision, los sintomas,\"\n",
    "                \"el diagnotico y  el tratamiento recomendado. Los sintomas deben estar por separado. Si hay mas de un tratamiento debe estar por separado. La fecha transformarla a formato dd/mm/aaaa.\" \n",
    "                \"Si alguna de estas entidades no se encuentran en el texto no inventes informacion.  Dar la respuesta en formato JSON\"},\n",
    "              {\"role\": \"user\", \"content\": f\"{text_to_analize}\"}],\n",
    ")\n",
    "\n",
    "llm_response=response.message.content[0].text\n",
    "print(llm_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f6f0009-8a9c-49a3-b740-d73e0dbe765e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paciente': {'nombre': 'Sof√≠a L√≥pez', 'edad': '28 a√±os'},\n",
       " 'fecha_admision': '03/04/2023',\n",
       " 'sintomas': ['Fiebre alta', 'Tos persistente'],\n",
       " 'diagnostico': 'Neumon√≠a',\n",
       " 'tratamiento': ['Antibi√≥ticos', 'Reposo']}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "final_result = json.loads(llm_response)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37fbf25-6db4-432a-82c4-2e7edce27686",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Tenemos dos funciones en Python, una llamada *'add_contact'* y otra llamada *'get_information'*.\n",
    "\n",
    "**Utilizar algun LLM que permita funtion calling** y desarrollar un codigo secuencial automatico que consiga:\n",
    "\n",
    "Interpretar la consulta del usuario, identificar a que funcion llamar, luego llamarla (si es que aplica) y darle una respuesta final al usuario.  (usar function calling para esta solucion)\n",
    "\n",
    "La entrada a dicho codigo es la consulta del usuario, a continuacion algunos ejemplos:\n",
    "\n",
    "- \"Agrega a Juan P√©rez con el n√∫mero 555-1234 y el correo juanperez@mail.com.\"\n",
    "- \"Guarda a Luc√≠a G√≥mez en mis contactos. Su tel√©fono es 555-5678 y su email es lucia.gomez@gmail.com.\"\n",
    "- \"Cual es el Email de Juan P√©rez.?\"\n",
    "\n",
    "Salidas esperadas de dichos ejemplos (variaran porque las genera el LLM):\n",
    "-  El contacto fue anadido con exito\n",
    "-  Se anadio el contacto\n",
    "-  El email de juan perez es juanperez@mail.com\n",
    "\n",
    "Link de ayuda: https://github.com/cohere-ai/notebooks/blob/main/notebooks/agents/Vanilla_Tool_Use_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6e43cb9-5e6a-4807-9818-c85408f1ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_contact(name, phone, email):\n",
    "    \"\"\"\n",
    "    Agrega un contacto al diccionario.\n",
    "    Par√°metros:\n",
    "        name (str): Nombre del contacto.\n",
    "        phone (str): N√∫mero de tel√©fono del contacto.\n",
    "        email (str): Correo electr√≥nico del contacto.\n",
    "    Retorna:\n",
    "        str: Mensaje confirmando la adici√≥n del contacto.\n",
    "    \"\"\"\n",
    "    contacts[name] = {'phone': phone, 'email': email}\n",
    "    return \"Contacto a√±adido con √©xito.\"\n",
    "\n",
    "def get_information(name):\n",
    "    \"\"\"\n",
    "    Recupera la informaci√≥n de un contacto.\n",
    "    Par√°metros:\n",
    "        name (str): Nombre del contacto.\n",
    "    Retorna:\n",
    "        dict/str: Informaci√≥n del contacto o un mensaje si no existe.\n",
    "    \"\"\"\n",
    "    if name in contacts:\n",
    "        return contacts[name]\n",
    "    else:\n",
    "        return \"Contacto no encontrado.\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fb79ea5-4ae3-4367-8213-8c38059b3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts = {\n",
    "                        'Joaquin Lopez':{'phone': 15456663258, 'email': 'Joacolocolopez@gmail.com'},\n",
    "                      'Flavio Oncativo':{'phone': 1545554178, 'email': 'FOncativo@hotmail.com'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c36ef67d-dd54-41e7-bef5-f89ee575603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIPS\n",
    "# Probar primero generando una funcion y llamarla, luego anadir la otra\n",
    "# Plantearlo paso por paso en distintas celdas, analizar las salidas y las entradas, como identificamos a que funcion llamar?\n",
    "# luego automatizar dentro de una sola celda\n",
    "\n",
    "\n",
    "# Lo importante es entregar hasta donde lleguen, sea una funcion, las dos pero sin poder hacer el flujo automatico, lo que puedan, siempre y cuando este\n",
    "# claro lo que se quizo hacer con comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db7ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicion del esquema de herramientas \n",
    "\n",
    "\n",
    "function_map={\"add_contact\":add_contact,\n",
    "              \"get_information\": get_information}\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {  \n",
    "            \"name\": \"add_contact\",\n",
    "            \"description\": \"Add contact to dict and returns a message confirming the addition of the contact\",\n",
    "            \"parameters\": {  \n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"description\": \"Name of new contact\"\n",
    "                    },\n",
    "                    \"phone\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"phone number of new contact\"\n",
    "                    },\n",
    "                    \"email\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"email of new contact\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"name\", \"phone\", \"email\"]  #\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_information\",\n",
    "            \"description\": \"get information about of a contact. If request \",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Name of contact\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"name\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07007ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El n√∫mero de Joaquin Lopez es 15456663258.\n"
     ]
    }
   ],
   "source": [
    "# Definicion de promt y mensaje al modelo\n",
    "system_promt=f\"\"\"Sos un asistente de agenda capaza de interpretar los pedidos. Tenes a dispocicion estas tools: {tools}\n",
    "y este es el diccionario de contactos que tenes que usar {contacts}. Solo responde a los pedidos de agregar un contacto o consultas sobre la informaci√≥n de algun contacto que se encuentre en el diccionario brindado. Si el usuario pide el telefono solo brindarle el campo \"phone\", si pide el mail o email dar el campo \"mail\" y si no especifica brindar ambos. \"\"\"\n",
    "\n",
    "messages=[{\"role\": \"system\", \"content\": f\"{system_promt}\"},\n",
    "              {\"role\": \"user\", \"content\": \"dame e numero de Joaquin Lopez\"}]\n",
    "\n",
    "# llamada al modelo con la solicitud del usuario\n",
    "co = cohere.ClientV2()\n",
    "response = co.chat(\n",
    "    model=\"command-a-03-2025\",\n",
    "    messages=messages,\n",
    "    tools=tools, \n",
    ")\n",
    "\n",
    "    \n",
    "# busqueda de la funcion a utilizar (si la necesita) y guardado de la salida en tool_content\n",
    "if response.message.tool_calls:\n",
    "    messages.append(response.message)\n",
    "    for tc in response.message.tool_calls:\n",
    "\n",
    "        tool_result = function_map[tc.function.name](\n",
    "            **json.loads(tc.function.arguments)\n",
    "        )\n",
    "      \n",
    "        tool_content = []\n",
    "        for data in tool_result:\n",
    "            tool_content.append(\n",
    "                {\n",
    "                    \"type\": \"document\",\n",
    "                    \"document\": {\"data\": json.dumps(data)},\n",
    "                }\n",
    "            )\n",
    "       \n",
    "       # agrega la informaci√≥n recuperada de la herramienta\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tc.id,\n",
    "                \"content\": tool_content,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Generacion de respuesta con la informacion recuperada\n",
    "\n",
    "response = co.chat(\n",
    "    model=\"command-a-03-2025\", messages=messages, tools=tools\n",
    ")\n",
    "print(response.message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af0a526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Joaquin Lopez': {'phone': 15456663258, 'email': 'Joacolocolopez@gmail.com'},\n",
       " 'Flavio Oncativo': {'phone': 1545554178, 'email': 'FOncativo@hotmail.com'},\n",
       " 'Juan P√©rez': {'phone': '555-1234', 'email': 'juanperez@mail.com'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8badf-dc90-4005-b916-8e528105d797",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "\n",
    "Crear una funcion llamada \"history_answer\", que toma como parametro de entrada una pregunta sobre un contexto dado y la salida es la respuesta final del proceso impulsado por un LLM.\n",
    "\n",
    "Dada una historia, el usuario podra hacer preguntas sobre la misma y el LLM debe responder siguiendo los siguientes lineamientos:\n",
    "\n",
    "REQUISITOS DE LA RESPUESTA\n",
    "- las respuestas deben ser en base a la historia\n",
    "- ante la misma pregunta siempre debe responder de la misma manera.\n",
    "- que responda en solo una oracion.\n",
    "- el idioma que responde debe ser el mismo que con el que se pregunta (ingles, espanol, portugues).\n",
    "- que agregue emojis en la oracion que resuman el contenido de la misma.\n",
    "- que responda siempre en tercera persona.\n",
    "- si la pregunta no tiene relacion alguna con el contexto, la respuesta debe ser 'Lo siento no puedo ayudarte con eso'.\n",
    "- Responder con 'Hakuna Matata!' al final de **todas** las respuestas (no importa idioma ni cantidad de tokens).\n",
    "\n",
    "**Ayudin**: \n",
    "- No se limiten a usar 1 solo request al LLM, pueden dividirlo en partes para que por un lado se verifique el idioma, por otro lado se verifique si la pregunta tiene relacion con el contexto, etc\n",
    "\n",
    "- Estructuren bien el prompt procurando separar instrucciones, contexto(historia) y pregunta del usuario.\n",
    "\n",
    "- Recuerden usar el system message y user message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "09f36820-b7d3-4813-a0c3-351c598106ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo flojo de estructura de prompt\n",
    "# prompt = f\"Responde a la pregunta: {pregunta} de manera concisa y divertida en base a la siguiente historia: {historia}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ecaf7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "historia = \"\"\"En un peque√±o feudo medieval, Thomas, un joven campesino de diecis√©is a√±os, trabajaba desde el amanecer en los campos de trigo del se√±or feudal. El sol apenas hab√≠a salido cuando √©l ya hab√≠a arado m√°s de lo que sus manos pod√≠an soportar. La vida era dura, pero su familia depend√≠a de la cosecha para pagar los impuestos y mantener su hogar de madera y paja.\n",
    "\n",
    "Un d√≠a, el feudo fue sacudido por noticias de guerra. El rey hab√≠a llamado a todos los hombres en edad de luchar. Thomas sab√≠a que, al igual que otros j√≥venes, no ten√≠a elecci√≥n. Cambi√≥ la hoz por una lanza rudimentaria y se uni√≥ a la milicia local. Sin entrenamiento, fue empujado a un campo de batalla embarrado, donde el acero resonaba y los gritos de los hombres llenaban el aire.\n",
    "\n",
    "La batalla fue un caos. Thomas, con el coraz√≥n latiendo en su pecho como un tambor de guerra, apenas pod√≠a distinguir amigo de enemigo. Logr√≥ esquivar una espada, pero cay√≥ al suelo, cubierto de lodo y sangre. Levant√°ndose, vio c√≥mo un compa√±ero ca√≠a junto a √©l, sus ojos abiertos, vac√≠os.\n",
    "\n",
    "Cuando la batalla termin√≥, el silencio era tan profundo como el vac√≠o que sent√≠a. Thomas regres√≥ al feudo, diferente, marcado por la muerte y la violencia. Su madre lo recibi√≥ con l√°grimas en los ojos, pero √©l, con la mirada fija en el horizonte, sab√≠a que la inocencia hab√≠a quedado atr√°s, enterrada en aquel campo de batalla. La paz del feudo ya no era la misma; √©l tampoco.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "254afcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def language_detect(pregunta,respuesta):\n",
    "    \n",
    "    \"\"\" Traduce una respuesta con un modelo de traduccion\n",
    "    Argumentos:\n",
    "        pregunta (str): entrada del usuario al LLM\n",
    "        respuesta (str): respuesta del primer LLM en espa√±ol\n",
    "    Retorna:\n",
    "        respuesta_trad (str): respuesta traducida por el LLM .          \n",
    "    \n",
    "    ----NOTA----: \n",
    "        Falta optimizar esta funcion, ya que si el lenguaje es espa√±ol no deber√≠a pasar por este modelo.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Definicion del promt para el modelo de traduccion \n",
    "    system_promt_DL= f\"\"\"Sos un asistente con capacidad de interpretar y traducir cuando sea necesario. Tu funcion es responder siempre en el mismo idioma de la pregunta.\n",
    "    Estilo de respuesta: \n",
    "    - Solo dar la traduccion de {respuesta}\n",
    "    - Mantener los emojis que resumen el contenido y la expresion \"Hakuna Matata!\" al finalizar la oracion\n",
    "    \n",
    " \n",
    "    Pregunta del usuario: {pregunta}\"\"\"\n",
    "    \n",
    "    \n",
    "    # Llamada al modelo para traducir. \n",
    "    #       seed=1212, fija la semilla que normalmente es aleatoria y permite resultados repetibles\n",
    "    #       temperature=0, fuerza al modelo a ser exacto y repetitivo\n",
    "    #       k=0, valor que controla la cantidad de tokens probables, se queda solo con el mas probable\n",
    "    #       p=0, valor que limita al modelo para eligir solo el token mas probable\n",
    "    co = cohere.ClientV2()\n",
    "    response = co.chat(\n",
    "        model= \"command-a-translate-08-2025\",\n",
    "        messages= [{\"role\": \"system\", \"content\": f\"{system_promt_DL}\"},\n",
    "                   {\"role\": \"user\", \"content\": f\"{pregunta}\"}],\n",
    "        temperature=0, \n",
    "        seed=1212,\n",
    "        k=0,\n",
    "        p=0\n",
    "    )\n",
    "    \n",
    "    # Obtiene la respuesta\n",
    "    respuesta_trad= response.message.content[0].text\n",
    "      \n",
    "    return respuesta_trad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4fa65537-3aa6-43c6-87e9-86a689f8e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion del promt para el modelo que genera la respuesta\n",
    "\n",
    "system_promt= f\"\"\" Eres un asistente con capacidad de interpretar el contexto brindado. Tu funcion es responder solo sobre el contenido del contexto dado, si la pregunta no tiene relacion con el contexto la respuesta debe ser \"Lo siento, no puedo ayudarte con eso\". \n",
    "Idioma:\n",
    "Responde en el idioma de entrada al modelo \n",
    "Estilo de respuesta:\n",
    "- Extension maxima: 1 oraci√≥n \n",
    "- Responde siempre en tercera persona. \n",
    "- Utiliza la expresion \"Hakuna Matata!\" al final de todas las respuestas.\n",
    "\n",
    "\n",
    " Contexto: {historia} \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def history_answer(pregunta):\n",
    "    \"\"\" Responde a una pregunta del contexto siguiendo las especificaciones del promt \n",
    "    Argumentos:\n",
    "        pregunta (str): entrada del usuario al LLM\n",
    "    Retorna: \n",
    "        respuesta_al_usuario (str): respuesta traducida \n",
    "\n",
    "    NOTA:\n",
    "        Una opcion para optimizar la obtenci√≥n de resultados iguales ante la misma pregunta, es utilizar un archivo JSON de forma local que guarde la pregunta y su respuesta, o bien guardarlas en la memoria cache. El problema de estas practicas radica en que la pregunta debe ser exactamente igual para que esto funcione, sino se le vuelve a consultar al modelo. Para el fin de este ejercicio, y por el tiempo con el que contaba, decidi que el uso de los parametros semilla, temperatura, top-k y top-p para forzar la respuesta era lo mas adecuado. En las pruebas realizadas a veces la respuesta var√≠a ligeramente, otras veces es la misma. \n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "    # \n",
    "    #Llamada al LLM para responder la pregunta\n",
    "    #       seed=1212, fija la semilla que normalmente es aleatoria y permite resultados repetibles\n",
    "    #       temperature=0, fuerza al modelo a ser exacto y repetitivo\n",
    "    #       k=0, valor que controla la cantidad de tokens probables, se queda solo con el mas probable\n",
    "    #       p=0, valor que limita al modelo para eligir solo el token mas probable\n",
    "    co = cohere.ClientV2()\n",
    "    response = co.chat(\n",
    "    model=\"command-a-03-2025\",\n",
    "    messages=[{\"role\": \"system\", \"content\": f\"{system_promt}\"},\n",
    "         {\"role\": \"user\", \"content\": f\"{pregunta}\"}],\n",
    "    temperature=0, \n",
    "    seed=1212,\n",
    "    k=0,\n",
    "    p=0 \n",
    "    )\n",
    "       \n",
    "    # Manda la respuesta a la funcion para detectar el lenguaje y traducir la respuesta      \n",
    "    respuesta_al_usuario=language_detect(pregunta, response.message.content[0].text)\n",
    "    \n",
    "    return respuesta_al_usuario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "026784c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas fue reclutado para la guerra, luch√≥ en una batalla ca√≥tica, presenci√≥ la muerte y regres√≥ marcado por la violencia, perdiendo su inocencia. üò¢üòî Hakuna Matata!\n"
     ]
    }
   ],
   "source": [
    "pregunta=\"\"\"que le paso a thomas?\"\"\"\n",
    "\n",
    "print(history_answer(pregunta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e9a7d-aa39-4a01-ba55-9a7f4ea39522",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Crear un chatbot sencillo impulsado por un LLM. \n",
    "\n",
    "Dicho bot esta destinado a un usuario final y debe cumplir las siguientes **condiciones en sus respuestas**:\n",
    "\n",
    "- Responder en no mas de 70 tokens.\n",
    "- Responder de manera positiva, con un tono entusiasta.\n",
    "- Responder con consejos √∫tiles, como si fueras un tutor.\n",
    "\n",
    " \n",
    "**Otras consideraciones**:\n",
    "\n",
    "Respetar el formato de la interfaz provista por el ejercicio.\n",
    "\n",
    "Ademas agregar al codigo propuesto un historial de conversaciones para que el bot pueda mantener el hilo de lo que se esta hablando. Para probar no usen mas de 3 conversaciones anidadas para no enviarle tantos tokens.\n",
    "\n",
    "Dejar impreso en el notebook el historial de la conversacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa475fa8-e48b-423e-9006-7478a462129c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4096990b2074718a6ec41366f6f32b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Escribe tu mensaje aqu√≠...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70531e92df0140638a760025469b6a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Enviar', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f13b104cac493ea89580c45df437cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Definicion del promt y mensaje para el sistema\n",
    "system_promt= \"\"\" Eres un asistente personal. Tu objetivo es responder con consejos utiles como si fueras un tutor. Mantene un tono entusiasta y responde siempre de manera positiva. \n",
    "Idioma:\n",
    "respondes exclusivamente en espa√±ol castellano rioplatense.\n",
    "Estilo de respuesta:\n",
    "- extension maxima: 70 tokens\n",
    "- nunca inventes informacion\n",
    "\"\"\"\n",
    "system_msg= [{\"role\": \"system\", \"content\": f\"{system_promt}\"}]\n",
    "\n",
    "# variable que contendra hasta 3 preguntas con sus respuestas\n",
    "chat_history=deque(maxlen=6)   \n",
    "\n",
    "\n",
    "# Crear widgets de entrada y salida\n",
    "input_box = widgets.Text(placeholder='Escribe tu mensaje aqu√≠...')\n",
    "send_button = widgets.Button(description='Enviar')\n",
    "output_box = widgets.Output()     \n",
    "\n",
    "# Funci√≥n de respuesta simulada del chatbot\n",
    "def chatbot_response(message):\n",
    "    global chat_history\n",
    "    \n",
    "    \n",
    "    user = {\"role\": \"user\", \"content\": f\"{message}\"}\n",
    "    chat_history.append(user)\n",
    "    \n",
    "    messages= system_msg + list(chat_history)\n",
    "    \n",
    "    # Conexion al modelo\n",
    "    co = cohere.ClientV2()\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        messages=messages,\n",
    "        max_tokens=70,\n",
    "        temperature=0.5)\n",
    " \n",
    " # guarda la pregunta y respuesta en el historial\n",
    "    chat_history.append(response.message)\n",
    "        \n",
    "    rta_usuario= response.message.content[0].text\n",
    "     \n",
    "    return (rta_usuario)\n",
    "\n",
    "# Funci√≥n de manejo del bot√≥n\n",
    "def on_send_button_clicked(b):\n",
    "    with output_box:\n",
    "        clear_output(wait=True)\n",
    "        user_message = input_box.value\n",
    "        if user_message.strip():\n",
    "            print(f\"T√∫: {user_message}\")\n",
    "            response = chatbot_response(user_message)\n",
    "            print(f\"Chatbot: {response}\")\n",
    "        input_box.value = ''\n",
    "\n",
    "# Asociar funci√≥n al bot√≥n\n",
    "send_button.on_click(on_send_button_clicked)\n",
    "\n",
    "# Mostrar widgets\n",
    "display(input_box, send_button, output_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c7aaf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'que puedo hacer en cordoba?'}\n",
      "role='assistant' tool_calls=None tool_plan=None content=[TextAssistantMessageResponseContentItem(type='text', text='¬°C√≥rdoba es una ciudad incre√≠ble con muchas opciones para disfrutar! Aqu√≠ te doy algunas ideas de actividades que pod√©s hacer:\\n\\n- Visitar el Centro Hist√≥rico: Caminar por las calles del centro es una experiencia √∫nica. Pod√©s explorar la Catedral, el Cabildo, la Manzana Jesu√≠tica (declarada Patrimonio de la Humanidad')] citations=None\n",
      "{'role': 'user', 'content': 'may museos?'}\n",
      "role='assistant' tool_calls=None tool_plan=None content=[TextAssistantMessageResponseContentItem(type='text', text='¬°S√≠, C√≥rdoba tiene una gran oferta de museos para visitar! Te menciono algunos de los m√°s destacados:\\n\\n- Museo Superior de Bellas Artes Evita: Ubicado en el Palacio Ferreyra, este museo exhibe una colecci√≥n de arte argentino y europeo.\\n- Museo Hist√≥rico de la Universidad Nacional de C√≥rdoba: Aqu√≠ pod√©s conocer la historia de')] citations=None\n",
      "{'role': 'user', 'content': 'hay de ciencia?'}\n",
      "role='assistant' tool_calls=None tool_plan=None content=[TextAssistantMessageResponseContentItem(type='text', text='¬°Por supuesto! C√≥rdoba es conocida por su importancia en el √°mbito cient√≠fico y tecnol√≥gico. Te recomiendo estos museos:\\n\\n- Museo de Ciencias Naturales de la Universidad Nacional de C√≥rdoba: Un lugar fascinante donde pod√©s explorar exposiciones de paleontolog√≠a, bot√°nica y zoolog√≠a.\\n- Museo de Mineralog√≠a y Geolog√≠a Dr. Alfredo Stelzner:')] citations=None\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(chat_history)):\n",
    "    print(chat_history[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb6d6a-1c32-42e5-a1ee-d62b2bc0785a",
   "metadata": {},
   "source": [
    "### RECOMENDACIONES GENERALES\n",
    "\n",
    "No se confien probando con un par de respuestas y ya, hagan minimo 5 pruebas por ejercicio para asi tener mas chances de visualizar errores en la generacion del contenido.\n",
    "\n",
    "Prueben combinar LLMs con programacion convencional para los casos que vean convenientes (decisiones if else, respuestas estaticas, etc)\n",
    "\n",
    "Prueben con distintos modelos de Cohere, hay algunos optimizados para ciertas aplicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f2e37-bf32-43b7-8958-e39954a20fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
